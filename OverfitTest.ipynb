{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "OverfitTest.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lungnahahd/Tensorflow_Prac/blob/main/OverfitTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Np0m3xGrif"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from collections import OrderedDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQmcGQjGrij"
      },
      "source": [
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coH1nfQhGril"
      },
      "source": [
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key] \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc8rUh6JGril"
      },
      "source": [
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JmIGqNcGriq"
      },
      "source": [
        "class MultiLayerNet:\n",
        "    \"\"\"완전연결 다층 신경망\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_list, output_size,\n",
        "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 계층 생성\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "                                                      self.params['b' + str(idx)])\n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "            self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\"가중치 초기화\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "        \"\"\"\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블 \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        손실 함수의 값\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(수치 미분).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6UogDkwGrit",
        "outputId": "d02abfa1-0948-4298-fb26-682820f6ab6e"
      },
      "source": [
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, train acc:0.14333333333333334, test acc:0.1277\n",
            "epoch:1, train acc:0.15666666666666668, test acc:0.1344\n",
            "epoch:2, train acc:0.16333333333333333, test acc:0.1428\n",
            "epoch:3, train acc:0.19333333333333333, test acc:0.1561\n",
            "epoch:4, train acc:0.20666666666666667, test acc:0.1685\n",
            "epoch:5, train acc:0.21333333333333335, test acc:0.176\n",
            "epoch:6, train acc:0.25, test acc:0.196\n",
            "epoch:7, train acc:0.2733333333333333, test acc:0.2154\n",
            "epoch:8, train acc:0.29, test acc:0.2359\n",
            "epoch:9, train acc:0.2966666666666667, test acc:0.2315\n",
            "epoch:10, train acc:0.3233333333333333, test acc:0.2524\n",
            "epoch:11, train acc:0.33666666666666667, test acc:0.2701\n",
            "epoch:12, train acc:0.37, test acc:0.2901\n",
            "epoch:13, train acc:0.38666666666666666, test acc:0.3078\n",
            "epoch:14, train acc:0.38666666666666666, test acc:0.3149\n",
            "epoch:15, train acc:0.4066666666666667, test acc:0.3309\n",
            "epoch:16, train acc:0.4266666666666667, test acc:0.3424\n",
            "epoch:17, train acc:0.44, test acc:0.3471\n",
            "epoch:18, train acc:0.44333333333333336, test acc:0.3591\n",
            "epoch:19, train acc:0.46, test acc:0.3693\n",
            "epoch:20, train acc:0.43333333333333335, test acc:0.3631\n",
            "epoch:21, train acc:0.44, test acc:0.3681\n",
            "epoch:22, train acc:0.4533333333333333, test acc:0.3779\n",
            "epoch:23, train acc:0.4666666666666667, test acc:0.3853\n",
            "epoch:24, train acc:0.45666666666666667, test acc:0.385\n",
            "epoch:25, train acc:0.47333333333333333, test acc:0.3925\n",
            "epoch:26, train acc:0.49, test acc:0.3953\n",
            "epoch:27, train acc:0.5166666666666667, test acc:0.4117\n",
            "epoch:28, train acc:0.5333333333333333, test acc:0.4231\n",
            "epoch:29, train acc:0.5533333333333333, test acc:0.4439\n",
            "epoch:30, train acc:0.5433333333333333, test acc:0.446\n",
            "epoch:31, train acc:0.57, test acc:0.4573\n",
            "epoch:32, train acc:0.62, test acc:0.4711\n",
            "epoch:33, train acc:0.5966666666666667, test acc:0.4608\n",
            "epoch:34, train acc:0.6166666666666667, test acc:0.4762\n",
            "epoch:35, train acc:0.6366666666666667, test acc:0.4851\n",
            "epoch:36, train acc:0.64, test acc:0.4941\n",
            "epoch:37, train acc:0.6666666666666666, test acc:0.503\n",
            "epoch:38, train acc:0.6566666666666666, test acc:0.5003\n",
            "epoch:39, train acc:0.6433333333333333, test acc:0.491\n",
            "epoch:40, train acc:0.6633333333333333, test acc:0.4976\n",
            "epoch:41, train acc:0.6566666666666666, test acc:0.4895\n",
            "epoch:42, train acc:0.68, test acc:0.5067\n",
            "epoch:43, train acc:0.7033333333333334, test acc:0.525\n",
            "epoch:44, train acc:0.7166666666666667, test acc:0.534\n",
            "epoch:45, train acc:0.71, test acc:0.5344\n",
            "epoch:46, train acc:0.7266666666666667, test acc:0.5499\n",
            "epoch:47, train acc:0.7033333333333334, test acc:0.543\n",
            "epoch:48, train acc:0.68, test acc:0.5532\n",
            "epoch:49, train acc:0.7233333333333334, test acc:0.571\n",
            "epoch:50, train acc:0.73, test acc:0.5702\n",
            "epoch:51, train acc:0.7366666666666667, test acc:0.5648\n",
            "epoch:52, train acc:0.75, test acc:0.5646\n",
            "epoch:53, train acc:0.7366666666666667, test acc:0.5636\n",
            "epoch:54, train acc:0.7333333333333333, test acc:0.5552\n",
            "epoch:55, train acc:0.74, test acc:0.5612\n",
            "epoch:56, train acc:0.73, test acc:0.5716\n",
            "epoch:57, train acc:0.7666666666666667, test acc:0.5863\n",
            "epoch:58, train acc:0.7766666666666666, test acc:0.5951\n",
            "epoch:59, train acc:0.7833333333333333, test acc:0.6162\n",
            "epoch:60, train acc:0.7933333333333333, test acc:0.6133\n",
            "epoch:61, train acc:0.8, test acc:0.6212\n",
            "epoch:62, train acc:0.8, test acc:0.6164\n",
            "epoch:63, train acc:0.7766666666666666, test acc:0.6016\n",
            "epoch:64, train acc:0.7766666666666666, test acc:0.6047\n",
            "epoch:65, train acc:0.8033333333333333, test acc:0.6199\n",
            "epoch:66, train acc:0.7866666666666666, test acc:0.6246\n",
            "epoch:67, train acc:0.8, test acc:0.6358\n",
            "epoch:68, train acc:0.7966666666666666, test acc:0.6385\n",
            "epoch:69, train acc:0.8, test acc:0.6392\n",
            "epoch:70, train acc:0.8066666666666666, test acc:0.6422\n",
            "epoch:71, train acc:0.8, test acc:0.6402\n",
            "epoch:72, train acc:0.8033333333333333, test acc:0.635\n",
            "epoch:73, train acc:0.79, test acc:0.6504\n",
            "epoch:74, train acc:0.7933333333333333, test acc:0.6256\n",
            "epoch:75, train acc:0.81, test acc:0.6244\n",
            "epoch:76, train acc:0.8133333333333334, test acc:0.6407\n",
            "epoch:77, train acc:0.8233333333333334, test acc:0.641\n",
            "epoch:78, train acc:0.8066666666666666, test acc:0.6493\n",
            "epoch:79, train acc:0.81, test acc:0.6437\n",
            "epoch:80, train acc:0.8066666666666666, test acc:0.6581\n",
            "epoch:81, train acc:0.8133333333333334, test acc:0.6566\n",
            "epoch:82, train acc:0.81, test acc:0.6488\n",
            "epoch:83, train acc:0.7966666666666666, test acc:0.6285\n",
            "epoch:84, train acc:0.8133333333333334, test acc:0.6418\n",
            "epoch:85, train acc:0.8066666666666666, test acc:0.6451\n",
            "epoch:86, train acc:0.8133333333333334, test acc:0.6481\n",
            "epoch:87, train acc:0.8066666666666666, test acc:0.6576\n",
            "epoch:88, train acc:0.8233333333333334, test acc:0.6479\n",
            "epoch:89, train acc:0.8166666666666667, test acc:0.6399\n",
            "epoch:90, train acc:0.8233333333333334, test acc:0.6562\n",
            "epoch:91, train acc:0.8266666666666667, test acc:0.6673\n",
            "epoch:92, train acc:0.82, test acc:0.65\n",
            "epoch:93, train acc:0.82, test acc:0.657\n",
            "epoch:94, train acc:0.83, test acc:0.6592\n",
            "epoch:95, train acc:0.8266666666666667, test acc:0.6653\n",
            "epoch:96, train acc:0.8166666666666667, test acc:0.6802\n",
            "epoch:97, train acc:0.82, test acc:0.6764\n",
            "epoch:98, train acc:0.8233333333333334, test acc:0.6833\n",
            "epoch:99, train acc:0.83, test acc:0.6824\n",
            "epoch:100, train acc:0.8366666666666667, test acc:0.6773\n",
            "epoch:101, train acc:0.84, test acc:0.6743\n",
            "epoch:102, train acc:0.8366666666666667, test acc:0.6763\n",
            "epoch:103, train acc:0.83, test acc:0.6748\n",
            "epoch:104, train acc:0.8266666666666667, test acc:0.6802\n",
            "epoch:105, train acc:0.8433333333333334, test acc:0.6682\n",
            "epoch:106, train acc:0.8266666666666667, test acc:0.6809\n",
            "epoch:107, train acc:0.8366666666666667, test acc:0.6704\n",
            "epoch:108, train acc:0.83, test acc:0.6829\n",
            "epoch:109, train acc:0.83, test acc:0.6772\n",
            "epoch:110, train acc:0.83, test acc:0.6786\n",
            "epoch:111, train acc:0.85, test acc:0.6784\n",
            "epoch:112, train acc:0.8466666666666667, test acc:0.6644\n",
            "epoch:113, train acc:0.8566666666666667, test acc:0.6844\n",
            "epoch:114, train acc:0.8433333333333334, test acc:0.6782\n",
            "epoch:115, train acc:0.8333333333333334, test acc:0.69\n",
            "epoch:116, train acc:0.8533333333333334, test acc:0.6781\n",
            "epoch:117, train acc:0.84, test acc:0.668\n",
            "epoch:118, train acc:0.8566666666666667, test acc:0.6708\n",
            "epoch:119, train acc:0.8466666666666667, test acc:0.6728\n",
            "epoch:120, train acc:0.8533333333333334, test acc:0.6874\n",
            "epoch:121, train acc:0.8433333333333334, test acc:0.6792\n",
            "epoch:122, train acc:0.8533333333333334, test acc:0.673\n",
            "epoch:123, train acc:0.85, test acc:0.6891\n",
            "epoch:124, train acc:0.8666666666666667, test acc:0.6872\n",
            "epoch:125, train acc:0.8566666666666667, test acc:0.6888\n",
            "epoch:126, train acc:0.8466666666666667, test acc:0.6925\n",
            "epoch:127, train acc:0.84, test acc:0.6975\n",
            "epoch:128, train acc:0.8533333333333334, test acc:0.6928\n",
            "epoch:129, train acc:0.85, test acc:0.6859\n",
            "epoch:130, train acc:0.8333333333333334, test acc:0.6821\n",
            "epoch:131, train acc:0.8566666666666667, test acc:0.6884\n",
            "epoch:132, train acc:0.8566666666666667, test acc:0.6891\n",
            "epoch:133, train acc:0.8466666666666667, test acc:0.6969\n",
            "epoch:134, train acc:0.8533333333333334, test acc:0.6828\n",
            "epoch:135, train acc:0.8566666666666667, test acc:0.6909\n",
            "epoch:136, train acc:0.8633333333333333, test acc:0.6919\n",
            "epoch:137, train acc:0.8566666666666667, test acc:0.6911\n",
            "epoch:138, train acc:0.8466666666666667, test acc:0.6938\n",
            "epoch:139, train acc:0.8533333333333334, test acc:0.6948\n",
            "epoch:140, train acc:0.86, test acc:0.6836\n",
            "epoch:141, train acc:0.8733333333333333, test acc:0.6916\n",
            "epoch:142, train acc:0.8666666666666667, test acc:0.6874\n",
            "epoch:143, train acc:0.8533333333333334, test acc:0.6831\n",
            "epoch:144, train acc:0.8533333333333334, test acc:0.6936\n",
            "epoch:145, train acc:0.8566666666666667, test acc:0.6999\n",
            "epoch:146, train acc:0.86, test acc:0.6999\n",
            "epoch:147, train acc:0.8566666666666667, test acc:0.692\n",
            "epoch:148, train acc:0.8533333333333334, test acc:0.7062\n",
            "epoch:149, train acc:0.86, test acc:0.6973\n",
            "epoch:150, train acc:0.8566666666666667, test acc:0.7024\n",
            "epoch:151, train acc:0.8633333333333333, test acc:0.6976\n",
            "epoch:152, train acc:0.8633333333333333, test acc:0.6981\n",
            "epoch:153, train acc:0.8666666666666667, test acc:0.6847\n",
            "epoch:154, train acc:0.8666666666666667, test acc:0.6993\n",
            "epoch:155, train acc:0.8733333333333333, test acc:0.6978\n",
            "epoch:156, train acc:0.8633333333333333, test acc:0.6896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:157, train acc:0.8633333333333333, test acc:0.6985\n",
            "epoch:158, train acc:0.87, test acc:0.6975\n",
            "epoch:159, train acc:0.8566666666666667, test acc:0.6976\n",
            "epoch:160, train acc:0.8633333333333333, test acc:0.6996\n",
            "epoch:161, train acc:0.88, test acc:0.7078\n",
            "epoch:162, train acc:0.8633333333333333, test acc:0.7089\n",
            "epoch:163, train acc:0.8633333333333333, test acc:0.7023\n",
            "epoch:164, train acc:0.87, test acc:0.6974\n",
            "epoch:165, train acc:0.86, test acc:0.6997\n",
            "epoch:166, train acc:0.8666666666666667, test acc:0.704\n",
            "epoch:167, train acc:0.8666666666666667, test acc:0.6978\n",
            "epoch:168, train acc:0.8666666666666667, test acc:0.7024\n",
            "epoch:169, train acc:0.8733333333333333, test acc:0.6998\n",
            "epoch:170, train acc:0.86, test acc:0.703\n",
            "epoch:171, train acc:0.8733333333333333, test acc:0.7012\n",
            "epoch:172, train acc:0.8733333333333333, test acc:0.6935\n",
            "epoch:173, train acc:0.8666666666666667, test acc:0.6943\n",
            "epoch:174, train acc:0.88, test acc:0.6986\n",
            "epoch:175, train acc:0.88, test acc:0.697\n",
            "epoch:176, train acc:0.88, test acc:0.7009\n",
            "epoch:177, train acc:0.87, test acc:0.7015\n",
            "epoch:178, train acc:0.8666666666666667, test acc:0.691\n",
            "epoch:179, train acc:0.8733333333333333, test acc:0.7037\n",
            "epoch:180, train acc:0.87, test acc:0.7042\n",
            "epoch:181, train acc:0.8633333333333333, test acc:0.7023\n",
            "epoch:182, train acc:0.8566666666666667, test acc:0.7079\n",
            "epoch:183, train acc:0.8566666666666667, test acc:0.7034\n",
            "epoch:184, train acc:0.87, test acc:0.7083\n",
            "epoch:185, train acc:0.8533333333333334, test acc:0.7043\n",
            "epoch:186, train acc:0.8633333333333333, test acc:0.701\n",
            "epoch:187, train acc:0.87, test acc:0.7049\n",
            "epoch:188, train acc:0.8666666666666667, test acc:0.705\n",
            "epoch:189, train acc:0.87, test acc:0.6951\n",
            "epoch:190, train acc:0.87, test acc:0.703\n",
            "epoch:191, train acc:0.8666666666666667, test acc:0.7\n",
            "epoch:192, train acc:0.87, test acc:0.705\n",
            "epoch:193, train acc:0.8733333333333333, test acc:0.7001\n",
            "epoch:194, train acc:0.8666666666666667, test acc:0.694\n",
            "epoch:195, train acc:0.8733333333333333, test acc:0.7031\n",
            "epoch:196, train acc:0.87, test acc:0.7025\n",
            "epoch:197, train acc:0.8666666666666667, test acc:0.7046\n",
            "epoch:198, train acc:0.86, test acc:0.7053\n",
            "epoch:199, train acc:0.88, test acc:0.702\n",
            "epoch:200, train acc:0.87, test acc:0.7037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkKhUpZuGriv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}