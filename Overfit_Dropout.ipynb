{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Overfit_Dropout.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lungnahahd/Tensorflow_Prac/blob/main/Overfit_Dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1RO1uVHADy"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from collections import OrderedDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiXhuHD2HAD2"
      },
      "source": [
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqEIaZlHAD3"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVskZZCvHAD8"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B72PPaRmHAD9"
      },
      "source": [
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKENXFiHAEA"
      },
      "source": [
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key] \n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():                                \n",
        "                self.v[key] = np.zeros_like(val)\n",
        "                \n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "    \n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7LHJ4sHAEC"
      },
      "source": [
        "class MultiLayerNetExtend:\n",
        "    \"\"\"완전 연결 다층 신경망(확장판)\n",
        "    가중치 감소, 드롭아웃, 배치 정규화 구현\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
        "    use_dropout : 드롭아웃 사용 여부\n",
        "    dropout_ration : 드롭아웃 비율\n",
        "    use_batchNorm : 배치 정규화 사용 여부\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_list, output_size,\n",
        "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
        "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 계층 생성\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "                                                      self.params['b' + str(idx)])\n",
        "            if self.use_batchnorm:\n",
        "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
        "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
        "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
        "                \n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "            \n",
        "            if self.use_dropout:\n",
        "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\"가중치 초기화\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "        \"\"\"\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for key, layer in self.layers.items():\n",
        "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t, train_flg=False):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블 \n",
        "        \"\"\"\n",
        "        y = self.predict(x, train_flg)\n",
        "\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay\n",
        "\n",
        "    def accuracy(self, X, T):\n",
        "        Y = self.predict(X, train_flg=False)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        if T.ndim != 1 : T = np.argmax(T, axis=1)\n",
        "\n",
        "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self, X, T):\n",
        "        \"\"\"기울기를 구한다(수치 미분).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_W = lambda W: self.loss(X, T, train_flg=True)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
        "            \n",
        "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
        "                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\n",
        "                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t, train_flg=True)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
        "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
        "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmHKASRqHAED"
      },
      "source": [
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        #if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGY07T7lHAEE",
        "outputId": "ee5f7719-f311-49bf-8e0a-b7f60ad1b212"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== epoch:1, train acc:0.17, test acc:0.1238 ===\n",
            "=== epoch:2, train acc:0.17333333333333334, test acc:0.1258 ===\n",
            "=== epoch:3, train acc:0.17333333333333334, test acc:0.1288 ===\n",
            "=== epoch:4, train acc:0.17333333333333334, test acc:0.1323 ===\n",
            "=== epoch:5, train acc:0.17333333333333334, test acc:0.1334 ===\n",
            "=== epoch:6, train acc:0.16666666666666666, test acc:0.135 ===\n",
            "=== epoch:7, train acc:0.17, test acc:0.1353 ===\n",
            "=== epoch:8, train acc:0.16333333333333333, test acc:0.1368 ===\n",
            "=== epoch:9, train acc:0.17, test acc:0.1389 ===\n",
            "=== epoch:10, train acc:0.17666666666666667, test acc:0.1394 ===\n",
            "=== epoch:11, train acc:0.17666666666666667, test acc:0.14 ===\n",
            "=== epoch:12, train acc:0.17666666666666667, test acc:0.1406 ===\n",
            "=== epoch:13, train acc:0.18666666666666668, test acc:0.1418 ===\n",
            "=== epoch:14, train acc:0.19, test acc:0.1431 ===\n",
            "=== epoch:15, train acc:0.19666666666666666, test acc:0.1458 ===\n",
            "=== epoch:16, train acc:0.2, test acc:0.15 ===\n",
            "=== epoch:17, train acc:0.21, test acc:0.1554 ===\n",
            "=== epoch:18, train acc:0.21, test acc:0.1581 ===\n",
            "=== epoch:19, train acc:0.22666666666666666, test acc:0.1621 ===\n",
            "=== epoch:20, train acc:0.23, test acc:0.168 ===\n",
            "=== epoch:21, train acc:0.23666666666666666, test acc:0.1779 ===\n",
            "=== epoch:22, train acc:0.23666666666666666, test acc:0.1813 ===\n",
            "=== epoch:23, train acc:0.24666666666666667, test acc:0.1828 ===\n",
            "=== epoch:24, train acc:0.25333333333333335, test acc:0.186 ===\n",
            "=== epoch:25, train acc:0.2733333333333333, test acc:0.1915 ===\n",
            "=== epoch:26, train acc:0.2866666666666667, test acc:0.2006 ===\n",
            "=== epoch:27, train acc:0.29333333333333333, test acc:0.2057 ===\n",
            "=== epoch:28, train acc:0.2866666666666667, test acc:0.2114 ===\n",
            "=== epoch:29, train acc:0.29333333333333333, test acc:0.2187 ===\n",
            "=== epoch:30, train acc:0.29333333333333333, test acc:0.2251 ===\n",
            "=== epoch:31, train acc:0.31333333333333335, test acc:0.2285 ===\n",
            "=== epoch:32, train acc:0.32, test acc:0.2409 ===\n",
            "=== epoch:33, train acc:0.33, test acc:0.2471 ===\n",
            "=== epoch:34, train acc:0.3233333333333333, test acc:0.2535 ===\n",
            "=== epoch:35, train acc:0.3233333333333333, test acc:0.2599 ===\n",
            "=== epoch:36, train acc:0.32666666666666666, test acc:0.2673 ===\n",
            "=== epoch:37, train acc:0.33, test acc:0.269 ===\n",
            "=== epoch:38, train acc:0.3466666666666667, test acc:0.2706 ===\n",
            "=== epoch:39, train acc:0.35, test acc:0.2746 ===\n",
            "=== epoch:40, train acc:0.3466666666666667, test acc:0.2725 ===\n",
            "=== epoch:41, train acc:0.3466666666666667, test acc:0.2726 ===\n",
            "=== epoch:42, train acc:0.35, test acc:0.2719 ===\n",
            "=== epoch:43, train acc:0.3466666666666667, test acc:0.2735 ===\n",
            "=== epoch:44, train acc:0.33666666666666667, test acc:0.2735 ===\n",
            "=== epoch:45, train acc:0.35, test acc:0.2751 ===\n",
            "=== epoch:46, train acc:0.3433333333333333, test acc:0.2754 ===\n",
            "=== epoch:47, train acc:0.3433333333333333, test acc:0.275 ===\n",
            "=== epoch:48, train acc:0.35333333333333333, test acc:0.2789 ===\n",
            "=== epoch:49, train acc:0.3566666666666667, test acc:0.2817 ===\n",
            "=== epoch:50, train acc:0.3566666666666667, test acc:0.2795 ===\n",
            "=== epoch:51, train acc:0.35, test acc:0.2758 ===\n",
            "=== epoch:52, train acc:0.35333333333333333, test acc:0.2732 ===\n",
            "=== epoch:53, train acc:0.3566666666666667, test acc:0.2763 ===\n",
            "=== epoch:54, train acc:0.35333333333333333, test acc:0.2748 ===\n",
            "=== epoch:55, train acc:0.35, test acc:0.2762 ===\n",
            "=== epoch:56, train acc:0.32666666666666666, test acc:0.2638 ===\n",
            "=== epoch:57, train acc:0.3433333333333333, test acc:0.2672 ===\n",
            "=== epoch:58, train acc:0.33666666666666667, test acc:0.2671 ===\n",
            "=== epoch:59, train acc:0.3333333333333333, test acc:0.266 ===\n",
            "=== epoch:60, train acc:0.33666666666666667, test acc:0.2635 ===\n",
            "=== epoch:61, train acc:0.32, test acc:0.2563 ===\n",
            "=== epoch:62, train acc:0.3233333333333333, test acc:0.258 ===\n",
            "=== epoch:63, train acc:0.33, test acc:0.2599 ===\n",
            "=== epoch:64, train acc:0.32, test acc:0.2548 ===\n",
            "=== epoch:65, train acc:0.33666666666666667, test acc:0.259 ===\n",
            "=== epoch:66, train acc:0.32666666666666666, test acc:0.2571 ===\n",
            "=== epoch:67, train acc:0.32666666666666666, test acc:0.259 ===\n",
            "=== epoch:68, train acc:0.32, test acc:0.2568 ===\n",
            "=== epoch:69, train acc:0.31333333333333335, test acc:0.2537 ===\n",
            "=== epoch:70, train acc:0.3233333333333333, test acc:0.2588 ===\n",
            "=== epoch:71, train acc:0.33666666666666667, test acc:0.2658 ===\n",
            "=== epoch:72, train acc:0.3333333333333333, test acc:0.2659 ===\n",
            "=== epoch:73, train acc:0.3333333333333333, test acc:0.2655 ===\n",
            "=== epoch:74, train acc:0.34, test acc:0.2695 ===\n",
            "=== epoch:75, train acc:0.3233333333333333, test acc:0.2619 ===\n",
            "=== epoch:76, train acc:0.3233333333333333, test acc:0.2596 ===\n",
            "=== epoch:77, train acc:0.3233333333333333, test acc:0.2544 ===\n",
            "=== epoch:78, train acc:0.32, test acc:0.2591 ===\n",
            "=== epoch:79, train acc:0.31666666666666665, test acc:0.2551 ===\n",
            "=== epoch:80, train acc:0.31666666666666665, test acc:0.2563 ===\n",
            "=== epoch:81, train acc:0.31, test acc:0.2527 ===\n",
            "=== epoch:82, train acc:0.30333333333333334, test acc:0.2492 ===\n",
            "=== epoch:83, train acc:0.31, test acc:0.2506 ===\n",
            "=== epoch:84, train acc:0.30666666666666664, test acc:0.2473 ===\n",
            "=== epoch:85, train acc:0.3, test acc:0.2451 ===\n",
            "=== epoch:86, train acc:0.3, test acc:0.2463 ===\n",
            "=== epoch:87, train acc:0.3, test acc:0.248 ===\n",
            "=== epoch:88, train acc:0.30666666666666664, test acc:0.2515 ===\n",
            "=== epoch:89, train acc:0.31, test acc:0.2519 ===\n",
            "=== epoch:90, train acc:0.31, test acc:0.2546 ===\n",
            "=== epoch:91, train acc:0.30666666666666664, test acc:0.2554 ===\n",
            "=== epoch:92, train acc:0.30666666666666664, test acc:0.257 ===\n",
            "=== epoch:93, train acc:0.31666666666666665, test acc:0.2586 ===\n",
            "=== epoch:94, train acc:0.3233333333333333, test acc:0.2665 ===\n",
            "=== epoch:95, train acc:0.32, test acc:0.263 ===\n",
            "=== epoch:96, train acc:0.32666666666666666, test acc:0.2642 ===\n",
            "=== epoch:97, train acc:0.32, test acc:0.2624 ===\n",
            "=== epoch:98, train acc:0.33, test acc:0.2661 ===\n",
            "=== epoch:99, train acc:0.3233333333333333, test acc:0.266 ===\n",
            "=== epoch:100, train acc:0.32666666666666666, test acc:0.2658 ===\n",
            "=== epoch:101, train acc:0.33666666666666667, test acc:0.2669 ===\n",
            "=== epoch:102, train acc:0.3333333333333333, test acc:0.2686 ===\n",
            "=== epoch:103, train acc:0.33666666666666667, test acc:0.2656 ===\n",
            "=== epoch:104, train acc:0.33, test acc:0.2692 ===\n",
            "=== epoch:105, train acc:0.32666666666666666, test acc:0.2706 ===\n",
            "=== epoch:106, train acc:0.33, test acc:0.2714 ===\n",
            "=== epoch:107, train acc:0.3233333333333333, test acc:0.2664 ===\n",
            "=== epoch:108, train acc:0.32666666666666666, test acc:0.2704 ===\n",
            "=== epoch:109, train acc:0.33, test acc:0.2679 ===\n",
            "=== epoch:110, train acc:0.3333333333333333, test acc:0.2685 ===\n",
            "=== epoch:111, train acc:0.32666666666666666, test acc:0.2647 ===\n",
            "=== epoch:112, train acc:0.3233333333333333, test acc:0.2622 ===\n",
            "=== epoch:113, train acc:0.32666666666666666, test acc:0.2685 ===\n",
            "=== epoch:114, train acc:0.32666666666666666, test acc:0.271 ===\n",
            "=== epoch:115, train acc:0.32666666666666666, test acc:0.2731 ===\n",
            "=== epoch:116, train acc:0.32666666666666666, test acc:0.2735 ===\n",
            "=== epoch:117, train acc:0.32666666666666666, test acc:0.2754 ===\n",
            "=== epoch:118, train acc:0.33666666666666667, test acc:0.2811 ===\n",
            "=== epoch:119, train acc:0.3333333333333333, test acc:0.2853 ===\n",
            "=== epoch:120, train acc:0.33666666666666667, test acc:0.2842 ===\n",
            "=== epoch:121, train acc:0.33666666666666667, test acc:0.2872 ===\n",
            "=== epoch:122, train acc:0.33666666666666667, test acc:0.2853 ===\n",
            "=== epoch:123, train acc:0.3466666666666667, test acc:0.2911 ===\n",
            "=== epoch:124, train acc:0.35333333333333333, test acc:0.299 ===\n",
            "=== epoch:125, train acc:0.35333333333333333, test acc:0.2992 ===\n",
            "=== epoch:126, train acc:0.35333333333333333, test acc:0.3017 ===\n",
            "=== epoch:127, train acc:0.35333333333333333, test acc:0.3077 ===\n",
            "=== epoch:128, train acc:0.3566666666666667, test acc:0.3023 ===\n",
            "=== epoch:129, train acc:0.35333333333333333, test acc:0.2983 ===\n",
            "=== epoch:130, train acc:0.35, test acc:0.3016 ===\n",
            "=== epoch:131, train acc:0.35333333333333333, test acc:0.3038 ===\n",
            "=== epoch:132, train acc:0.3433333333333333, test acc:0.301 ===\n",
            "=== epoch:133, train acc:0.35, test acc:0.3029 ===\n",
            "=== epoch:134, train acc:0.3566666666666667, test acc:0.3051 ===\n",
            "=== epoch:135, train acc:0.35333333333333333, test acc:0.3009 ===\n",
            "=== epoch:136, train acc:0.35, test acc:0.3001 ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "=== epoch:137, train acc:0.35, test acc:0.3072 ===\n",
            "=== epoch:138, train acc:0.36, test acc:0.3084 ===\n",
            "=== epoch:139, train acc:0.36, test acc:0.3146 ===\n",
            "=== epoch:140, train acc:0.3566666666666667, test acc:0.3124 ===\n",
            "=== epoch:141, train acc:0.36666666666666664, test acc:0.3164 ===\n",
            "=== epoch:142, train acc:0.38, test acc:0.3163 ===\n",
            "=== epoch:143, train acc:0.38, test acc:0.3203 ===\n",
            "=== epoch:144, train acc:0.38333333333333336, test acc:0.3266 ===\n",
            "=== epoch:145, train acc:0.37666666666666665, test acc:0.3252 ===\n",
            "=== epoch:146, train acc:0.37333333333333335, test acc:0.3204 ===\n",
            "=== epoch:147, train acc:0.37666666666666665, test acc:0.3269 ===\n",
            "=== epoch:148, train acc:0.3933333333333333, test acc:0.3267 ===\n",
            "=== epoch:149, train acc:0.4066666666666667, test acc:0.3275 ===\n",
            "=== epoch:150, train acc:0.41333333333333333, test acc:0.3357 ===\n",
            "=== epoch:151, train acc:0.4166666666666667, test acc:0.3376 ===\n",
            "=== epoch:152, train acc:0.41333333333333333, test acc:0.3377 ===\n",
            "=== epoch:153, train acc:0.3933333333333333, test acc:0.3359 ===\n",
            "=== epoch:154, train acc:0.4066666666666667, test acc:0.3405 ===\n",
            "=== epoch:155, train acc:0.41, test acc:0.3378 ===\n",
            "=== epoch:156, train acc:0.42333333333333334, test acc:0.3421 ===\n",
            "=== epoch:157, train acc:0.4266666666666667, test acc:0.3444 ===\n",
            "=== epoch:158, train acc:0.42333333333333334, test acc:0.3446 ===\n",
            "=== epoch:159, train acc:0.43333333333333335, test acc:0.3483 ===\n",
            "=== epoch:160, train acc:0.43, test acc:0.3482 ===\n",
            "=== epoch:161, train acc:0.43, test acc:0.3534 ===\n",
            "=== epoch:162, train acc:0.43, test acc:0.351 ===\n",
            "=== epoch:163, train acc:0.43333333333333335, test acc:0.3531 ===\n",
            "=== epoch:164, train acc:0.44666666666666666, test acc:0.3584 ===\n",
            "=== epoch:165, train acc:0.45, test acc:0.3627 ===\n",
            "=== epoch:166, train acc:0.44666666666666666, test acc:0.3566 ===\n",
            "=== epoch:167, train acc:0.44666666666666666, test acc:0.3625 ===\n",
            "=== epoch:168, train acc:0.44666666666666666, test acc:0.3616 ===\n",
            "=== epoch:169, train acc:0.45, test acc:0.3598 ===\n",
            "=== epoch:170, train acc:0.45, test acc:0.3566 ===\n",
            "=== epoch:171, train acc:0.4533333333333333, test acc:0.3608 ===\n",
            "=== epoch:172, train acc:0.4533333333333333, test acc:0.3615 ===\n",
            "=== epoch:173, train acc:0.45, test acc:0.3609 ===\n",
            "=== epoch:174, train acc:0.4533333333333333, test acc:0.3651 ===\n",
            "=== epoch:175, train acc:0.4533333333333333, test acc:0.3676 ===\n",
            "=== epoch:176, train acc:0.4533333333333333, test acc:0.3685 ===\n",
            "=== epoch:177, train acc:0.45666666666666667, test acc:0.3653 ===\n",
            "=== epoch:178, train acc:0.46, test acc:0.3657 ===\n",
            "=== epoch:179, train acc:0.4666666666666667, test acc:0.3726 ===\n",
            "=== epoch:180, train acc:0.45666666666666667, test acc:0.3715 ===\n",
            "=== epoch:181, train acc:0.46, test acc:0.3733 ===\n",
            "=== epoch:182, train acc:0.47, test acc:0.375 ===\n",
            "=== epoch:183, train acc:0.47, test acc:0.3772 ===\n",
            "=== epoch:184, train acc:0.47, test acc:0.3794 ===\n",
            "=== epoch:185, train acc:0.4666666666666667, test acc:0.3769 ===\n",
            "=== epoch:186, train acc:0.4633333333333333, test acc:0.3763 ===\n",
            "=== epoch:187, train acc:0.47, test acc:0.3789 ===\n",
            "=== epoch:188, train acc:0.47, test acc:0.3804 ===\n",
            "=== epoch:189, train acc:0.47, test acc:0.3821 ===\n",
            "=== epoch:190, train acc:0.47, test acc:0.3822 ===\n",
            "=== epoch:191, train acc:0.4766666666666667, test acc:0.3875 ===\n",
            "=== epoch:192, train acc:0.48, test acc:0.3889 ===\n",
            "=== epoch:193, train acc:0.47333333333333333, test acc:0.3905 ===\n",
            "=== epoch:194, train acc:0.4766666666666667, test acc:0.3889 ===\n",
            "=== epoch:195, train acc:0.4766666666666667, test acc:0.3891 ===\n",
            "=== epoch:196, train acc:0.48, test acc:0.3914 ===\n",
            "=== epoch:197, train acc:0.48, test acc:0.3924 ===\n",
            "=== epoch:198, train acc:0.48, test acc:0.3943 ===\n",
            "=== epoch:199, train acc:0.48, test acc:0.3946 ===\n",
            "=== epoch:200, train acc:0.48, test acc:0.3977 ===\n",
            "=== epoch:201, train acc:0.4766666666666667, test acc:0.3964 ===\n",
            "=== epoch:202, train acc:0.4766666666666667, test acc:0.3928 ===\n",
            "=== epoch:203, train acc:0.4766666666666667, test acc:0.391 ===\n",
            "=== epoch:204, train acc:0.48, test acc:0.3919 ===\n",
            "=== epoch:205, train acc:0.48333333333333334, test acc:0.3944 ===\n",
            "=== epoch:206, train acc:0.4866666666666667, test acc:0.3984 ===\n",
            "=== epoch:207, train acc:0.4866666666666667, test acc:0.4023 ===\n",
            "=== epoch:208, train acc:0.4866666666666667, test acc:0.4006 ===\n",
            "=== epoch:209, train acc:0.48333333333333334, test acc:0.4046 ===\n",
            "=== epoch:210, train acc:0.49, test acc:0.4051 ===\n",
            "=== epoch:211, train acc:0.49333333333333335, test acc:0.4067 ===\n",
            "=== epoch:212, train acc:0.49666666666666665, test acc:0.4083 ===\n",
            "=== epoch:213, train acc:0.5, test acc:0.4097 ===\n",
            "=== epoch:214, train acc:0.49666666666666665, test acc:0.4074 ===\n",
            "=== epoch:215, train acc:0.5033333333333333, test acc:0.4099 ===\n",
            "=== epoch:216, train acc:0.5033333333333333, test acc:0.41 ===\n",
            "=== epoch:217, train acc:0.51, test acc:0.4111 ===\n",
            "=== epoch:218, train acc:0.5133333333333333, test acc:0.4114 ===\n",
            "=== epoch:219, train acc:0.51, test acc:0.4142 ===\n",
            "=== epoch:220, train acc:0.5133333333333333, test acc:0.4134 ===\n",
            "=== epoch:221, train acc:0.5166666666666667, test acc:0.4173 ===\n",
            "=== epoch:222, train acc:0.5166666666666667, test acc:0.4201 ===\n",
            "=== epoch:223, train acc:0.52, test acc:0.42 ===\n",
            "=== epoch:224, train acc:0.52, test acc:0.4215 ===\n",
            "=== epoch:225, train acc:0.52, test acc:0.423 ===\n",
            "=== epoch:226, train acc:0.53, test acc:0.4235 ===\n",
            "=== epoch:227, train acc:0.5233333333333333, test acc:0.4247 ===\n",
            "=== epoch:228, train acc:0.5166666666666667, test acc:0.4203 ===\n",
            "=== epoch:229, train acc:0.5233333333333333, test acc:0.4203 ===\n",
            "=== epoch:230, train acc:0.5233333333333333, test acc:0.4214 ===\n",
            "=== epoch:231, train acc:0.5266666666666666, test acc:0.4234 ===\n",
            "=== epoch:232, train acc:0.5233333333333333, test acc:0.4215 ===\n",
            "=== epoch:233, train acc:0.5333333333333333, test acc:0.4252 ===\n",
            "=== epoch:234, train acc:0.5266666666666666, test acc:0.4265 ===\n",
            "=== epoch:235, train acc:0.5233333333333333, test acc:0.425 ===\n",
            "=== epoch:236, train acc:0.53, test acc:0.4267 ===\n",
            "=== epoch:237, train acc:0.5366666666666666, test acc:0.4267 ===\n",
            "=== epoch:238, train acc:0.5366666666666666, test acc:0.4281 ===\n",
            "=== epoch:239, train acc:0.5366666666666666, test acc:0.4298 ===\n",
            "=== epoch:240, train acc:0.54, test acc:0.4345 ===\n",
            "=== epoch:241, train acc:0.5466666666666666, test acc:0.4379 ===\n",
            "=== epoch:242, train acc:0.54, test acc:0.4409 ===\n",
            "=== epoch:243, train acc:0.5566666666666666, test acc:0.4463 ===\n",
            "=== epoch:244, train acc:0.5466666666666666, test acc:0.4467 ===\n",
            "=== epoch:245, train acc:0.55, test acc:0.4468 ===\n",
            "=== epoch:246, train acc:0.5466666666666666, test acc:0.4484 ===\n",
            "=== epoch:247, train acc:0.5533333333333333, test acc:0.4486 ===\n",
            "=== epoch:248, train acc:0.5566666666666666, test acc:0.4504 ===\n",
            "=== epoch:249, train acc:0.5533333333333333, test acc:0.4502 ===\n",
            "=== epoch:250, train acc:0.5533333333333333, test acc:0.4518 ===\n",
            "=== epoch:251, train acc:0.5533333333333333, test acc:0.4548 ===\n",
            "=== epoch:252, train acc:0.5566666666666666, test acc:0.453 ===\n",
            "=== epoch:253, train acc:0.56, test acc:0.4577 ===\n",
            "=== epoch:254, train acc:0.5666666666666667, test acc:0.4606 ===\n",
            "=== epoch:255, train acc:0.5666666666666667, test acc:0.4634 ===\n",
            "=== epoch:256, train acc:0.5666666666666667, test acc:0.4627 ===\n",
            "=== epoch:257, train acc:0.5766666666666667, test acc:0.4667 ===\n",
            "=== epoch:258, train acc:0.5766666666666667, test acc:0.4722 ===\n",
            "=== epoch:259, train acc:0.58, test acc:0.4729 ===\n",
            "=== epoch:260, train acc:0.58, test acc:0.4731 ===\n",
            "=== epoch:261, train acc:0.58, test acc:0.474 ===\n",
            "=== epoch:262, train acc:0.5866666666666667, test acc:0.479 ===\n",
            "=== epoch:263, train acc:0.5833333333333334, test acc:0.4779 ===\n",
            "=== epoch:264, train acc:0.5866666666666667, test acc:0.4803 ===\n",
            "=== epoch:265, train acc:0.5933333333333334, test acc:0.4826 ===\n",
            "=== epoch:266, train acc:0.6, test acc:0.488 ===\n",
            "=== epoch:267, train acc:0.6033333333333334, test acc:0.4887 ===\n",
            "=== epoch:268, train acc:0.6033333333333334, test acc:0.4924 ===\n",
            "=== epoch:269, train acc:0.5966666666666667, test acc:0.492 ===\n",
            "=== epoch:270, train acc:0.5933333333333334, test acc:0.494 ===\n",
            "=== epoch:271, train acc:0.5966666666666667, test acc:0.4947 ===\n",
            "=== epoch:272, train acc:0.6, test acc:0.5029 ===\n",
            "=== epoch:273, train acc:0.61, test acc:0.5057 ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "=== epoch:274, train acc:0.6066666666666667, test acc:0.5039 ===\n",
            "=== epoch:275, train acc:0.6066666666666667, test acc:0.5047 ===\n",
            "=== epoch:276, train acc:0.6133333333333333, test acc:0.5071 ===\n",
            "=== epoch:277, train acc:0.6166666666666667, test acc:0.5086 ===\n",
            "=== epoch:278, train acc:0.6133333333333333, test acc:0.5137 ===\n",
            "=== epoch:279, train acc:0.6133333333333333, test acc:0.5136 ===\n",
            "=== epoch:280, train acc:0.6133333333333333, test acc:0.5139 ===\n",
            "=== epoch:281, train acc:0.6133333333333333, test acc:0.5153 ===\n",
            "=== epoch:282, train acc:0.62, test acc:0.5162 ===\n",
            "=== epoch:283, train acc:0.6233333333333333, test acc:0.52 ===\n",
            "=== epoch:284, train acc:0.6233333333333333, test acc:0.5218 ===\n",
            "=== epoch:285, train acc:0.6333333333333333, test acc:0.5264 ===\n",
            "=== epoch:286, train acc:0.6333333333333333, test acc:0.5255 ===\n",
            "=== epoch:287, train acc:0.6333333333333333, test acc:0.5212 ===\n",
            "=== epoch:288, train acc:0.6466666666666666, test acc:0.5255 ===\n",
            "=== epoch:289, train acc:0.6433333333333333, test acc:0.5293 ===\n",
            "=== epoch:290, train acc:0.6466666666666666, test acc:0.5327 ===\n",
            "=== epoch:291, train acc:0.65, test acc:0.5328 ===\n",
            "=== epoch:292, train acc:0.6566666666666666, test acc:0.5346 ===\n",
            "=== epoch:293, train acc:0.6566666666666666, test acc:0.5368 ===\n",
            "=== epoch:294, train acc:0.6566666666666666, test acc:0.5334 ===\n",
            "=== epoch:295, train acc:0.66, test acc:0.537 ===\n",
            "=== epoch:296, train acc:0.6633333333333333, test acc:0.541 ===\n",
            "=== epoch:297, train acc:0.6733333333333333, test acc:0.5409 ===\n",
            "=== epoch:298, train acc:0.6666666666666666, test acc:0.5469 ===\n",
            "=== epoch:299, train acc:0.67, test acc:0.5482 ===\n",
            "=== epoch:300, train acc:0.6833333333333333, test acc:0.554 ===\n",
            "=== epoch:301, train acc:0.6966666666666667, test acc:0.5576 ===\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.5582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxEklEQVR4nO3deXxU1f3/8dcne0KQAAGEsAsFURAkItYVrQIuxZWqVVtrxVZttd9CxdpWrbVurVp/VRCVtlZxqeKOghugAmrY930LYcewJiHJnN8fM4EsM5MJzM027+fjwYOZO2fu/VxH7ufec8/5XHPOISIisSuurgMQEZG6pUQgIhLjlAhERGKcEoGISIxTIhARiXFKBCIiMc6zRGBm481sm5ktCvG5mdlTZrbKzBaY2clexSIiIqF5eUXwb2BImM+HAt0Df0YAYzyMRUREQvAsETjnpgO7wjQZBrzo/GYBGWbW1qt4REQkuIQ63HYWsLHc+9zAss2VG5rZCPxXDTRp0qR/z549ayVAEZHGYvbs2Tucc62CfVaXicCCLAta78I5Nw4YB5Cdne1ycnK8jEtEpNExs/WhPqvLUUO5QIdy79sDeXUUi4hIzKrLRPAucENg9NBAYLdzrkq3kIiIeMuzriEzewU4B8g0s1zgXiARwDk3FpgEXAisAg4AN3oVi4iIhOZZInDOXVPN5w64zavti4hIZDSzWEQkxikRiIjEOCUCEZEYp0QgIhLjlAhERGKcEoGISIxTIhARiXFKBCIiMU6JQEQkxikRiIjEOCUCEZEYp0QgIhLjlAhERGKcEoGISIxTIhARiXFKBCIiMU6JQEQkxikRiIjEOCUCEZEYp0QgIhLjlAhERGKcEoGISIxTIhARiXFKBCIiMU6JQEQkxikRiIjEOCUCEZEYp0QgIhLjlAhERGKcEoGISIxTIhARiXFKBCIiMU6JQEQkxikRiIjEOCUCEZEY52kiMLMhZrbczFaZ2eggnzczs/fMbL6ZLTazG72MR0REqvIsEZhZPPA0MBToBVxjZr0qNbsNWOKcOwk4B/i7mSV5FZOIiFTl5RXBAGCVc26Nc+4g8CowrFIbBzQ1MwPSgV1AiYcxiYhIJV4mgixgY7n3uYFl5f0TOB7IAxYCdzjnfJVXZGYjzCzHzHK2b9/uVbwiIjHJy0RgQZa5Su8HA/OAdkBf4J9mdkyVLzk3zjmX7ZzLbtWqVbTjFBGJaV4mglygQ7n37fGf+Zd3IzDR+a0C1gI9PYxJREQq8TIRfAt0N7MugRvAVwPvVmqzATgPwMzaAD2ANR7GJCIilSR4tWLnXImZ3Q5MBuKB8c65xWb2i8DnY4EHgH+b2UL8XUl3Oed2eBWTiIhU5VkiAHDOTQImVVo2ttzrPOACL2MQEZHwNLNYRCTGKRGIiMQ4JQIRkRinRCAiEuOUCEREYpwSgYhIjFMiEBGJcUoEIiIxTolARCTGKRGIiMQ4JQIRkRinRCAiEuOUCEREYpyn1UdFROTovT13E49NXk5efgHtMlIZNbgHl/ar/OTfI6dEICJSj709dxN3T1xIQXEpAJvyC7h74kKAqCUDdQ2JiNRjj01efigJlCkoLuWxycujtg0lAhGReiwvv6BGy4+EEoGISD3WLiO1RsuPhBKBiEg9NmpwD8wqLktNjGfU4B5R24YSgYhIPXZyx+Y4B8ekJGBAVkYqD13eW6OGREQau8V5u+mS2YT3FuQB8OGdZ5EVxe6g8pQIRETqmZVb93Lx//uSM7plsim/gFM6N/csCYASgYhIvTN22hqcgy9W7iDOYNQF0bsfEIwSgYhIHak8Y/i353dnw3cFvDU3l59+vzMdWqTRt0Mz+ndq4WkcSgQiInUg2Izh3/5vAQ64vF8Wowb3oEly7RyilQhEROpAsBnDDmielsjjP+pbq7EoEYiI1KKSUh8vzlwfcmZw/oHiWo5IiUBEpFZ9umwbf35/ScjPozljOFKaUCYiUovene+fF/D941qQnFDxEBztGcOR0hWBiIjH9hYW85f3l7LvYAmfLNnKDad14s/DTvT8OQORUiIQEfHYizPX81rORrq2akLXVun8+NROgP95AnVx4K9MiUBEJIrKn+Uf2yyFU7u04MtVOzjre6148WcD6jq8oJQIRESiwOdzvDs/r8LcgM27C3l7Xh7xBr8+t1sdRxiaEoGIyFEoLvXx6EfLmPD1BuLirMrcAIA2zVLI7uzt7OCj4emoITMbYmbLzWyVmY0O0eYcM5tnZovNbJqX8YiIRMO2vYX887OV5B84yGOTl/PcF2vp0CKNvYUlQdtvzi+s5QhrxrMrAjOLB54GzgdygW/N7F3n3JJybTKAZ4AhzrkNZtbaq3hERGrKOUfudwUck5rI58u2Her7T02K58DBUp6dvoZ9RSX8+NSO/OXSEzn1r5+ybW9RlfXUxdyAmvCya2gAsMo5twbAzF4FhgHlZ1JcC0x0zm0AcM5t8zAeEZGIbdtbyK9fmcusNbtIijfMjKISHwAHDpYSZ9CrbVP6tM/gN+d/DzPj9xceX+EeAdTd3ICa8LJrKAvYWO59bmBZed8DmpvZVDObbWY3BFuRmY0wsxwzy9m+fbtH4YqI+JX6HL+aMJf5G3dz15CelPjcoSRQxucg97tC7rmoF2lJ/nPqS/tl8dDlvcnKSPXsaWJe8PKKwIIsc0G23x84D0gFZprZLOfcigpfcm4cMA4gOzu78jpERKLquS/W8PXaXfztqpO4sn97HvloWdB2weoF1Ze5ATURUSIwszeB8cCHzjlfde0DcoEO5d63B/KCtNnhnNsP7Dez6cBJwApERGpB+XH/zdISueG0ToyZupqhJx7LFSf7D+hZGalsCnLQr+99/5GKtGtoDP7+/JVm9rCZ9YzgO98C3c2si5klAVcD71Zq8w5wppklmFkacCqwNMKYRESOyltzchk9cQGb8gtw+Ct/PvXpKpokJfDXy3pj5u/YGDW4B6mJ8RW+2xD6/iMV0RWBc+4T4BMzawZcA3xsZhuB54CXnHNV6qY650rM7HZgMhAPjHfOLTazXwQ+H+ucW2pmHwELAB/wvHNuUVT2TEQkhG17C/l6zS7umriQgyVVOzlSk+Jp3iTp0Puyrp76UBfIC+ZcZF3uZtYSuA64Hn8Xz8vAGUBv59w5XgVYWXZ2tsvJyamtzYlII3Td81/z5aodIT83YO3DF9VeQLXAzGY757KDfRZR15CZTQS+ANKAS5xzP3TOveac+xWQHr1QRUS8UVLqY+XWvXyyZCtfrtrBbYOOo22zlKBtG0vff6QiHTX0T+fcZ8E+CJVhRETqUvmbwG0zUmiSFM/KbfsBaJqSwC1nH0f31k3r/7j/x7rD/iBTrJq0hlEro7KJSBPB8WY2xzmXD2BmzYFrnHPPRCUKEZEoqvxg+LxAiYfBvVpzSd8surVO55iUxIbR9x8sCYRbfgQiTQQ3O+eeLnvjnPvOzG7GXx5CRKTWlD/TT0mMI7tTc/7784EArN+5n9++Pp/5ufkUl1a9/7kobw/P3nBKhWUNcdx/tEWaCOLMzFzgznKgjlBSNd8REYmqymf6BcU+vli1k/97bS5n92jNox8tZ29hcdAkAIevDKSiSOcRTAZeN7PzzOxc4BXgI+/CEhGp6rHJy4OWeZ44N487Xp3Hlj2F/OdnAxrHTeCCfHjhglrZVKRXBHcBtwC/xD+yagrwvFdBiYgEE6ykQ5nRQ3uSmZ5Mv47NuWtIzwZ8E7gVDLgFVk6GvLm1EkqkE8p8+GcXj/E2HBGJVeEe5D7+y7XMz80nOSGOwiATwLIyUvnF2ccdet+wbwJvh8//AmmZcNHf4bMHQ48aipJIaw11Bx4CegGHrrmcc12jFomIxJTdB4o5JjUBM6vS978pv4BRb8wHIC7O+PP7S2jbLIX05HiKfY5S3+F7AKHO9Bv0TeCrJ0DPwIS2/j/1fHORdg39C7gXeAIYBNxI8OqiIiIVBDvT79+pORc8MZ1rT+3IHy/uFbTvv7jU8ef3FlPic/TrmMH/bjmNhPi4sFcO9VZpCSyfBOmtocOpsOTt8O171u6s5kgTQapz7tPAyKH1wH1m9gX+5CAiElSwM/3RExfQv2NzCopLeeHLtZzerWXQyp4Auw4Uk5YUzxPD+5IQ7x/bUq/P9EP1+8cnQ2ngyWUdBsLGWbUbVzUiHTVUaGZx+KuP3m5mlwF6rKSIhBXsTL+w2MdXq3cyrG87urdO5+f/CV07LC0pnseHn0TnzCZehxodofr9S4vgvHvhzJH+JHDC5bUbVzUivSK4E3+doV8DD+DvHvqJRzGJSAO3a/9BXpq1Puwonz9c1Iutewq5fMwM+nfMYN7G3VVG+fz1st4MObFtbYR89PI3hP/8zP/z/93/J3BMe1j3pec3gSNVbSIITB4b7pwbBezDf39ARCSojbsOcOXYGWzdU/Uh7mWyMlJp1TSZVk2TmTn6XFo0SeKdeXkNr+8fwFcKaz6HSb+LrH1GR//fUaoTFA3VJgLnXKmZ9S8/s1hEpLJSn2PK4i08O30NB4pK+ddPT2H8V2v5Zu2uCs/7rTzKp2V6MtBA+/4TUqB5Z9i+DFIyajuqqIm0a2gu8I6Z/Q/YX7bQOTfRk6hEpF4rP3KnVdNkurVuQpfMdF7+egPxccbjw09iUM/WDOrZumGO8qksVN9/SSEkpcMVL0DPi+HBNrUbV5REmghaADuBc8stc4ASgUiMqTwSaNveIrbtLWLG6l1cclI7Hhh2AhlpFZ/u1eAO/DVx86eHXzdpXW/6/Wsi0pnFui8gIkDoej/HpCTw4GUnckxKYh1E5ZE9m2H6Y5G3r0f9/jUR6czif+G/AqjAOfezqEckIlEXafdMqHYfL9nKC1+uobjUhRzzv7ewpHElgSl/hBlP1XUUtSLSrqH3y71OAS7D/9xiEanngk3qunviQgAG9WzNrDU7cc7x9dqdvDxrAwcDJZzLJn+9nrOBGat30bllGu2bpxFn4AsybKRBVfYsE+4mcEkhnHQtDPg5PHdu1TaNSKRdQ2+Wf29mrwCfeBKRiERVsK6cguJSfv/WQpqlJrJ5d+ga/YXFPmas3sWIs7ry2wu+R3JCPBO+Xs+97y6uUPO/3lX2jFS4m8ADb4MLHoC4+Abb9x+pSK8IKusOdIxmICJy5AqLSw9N3mpzTApNkhPw+Rzrdx0IOanrwMFSurVO59Er+5CZnsyF//iiav9vwO8vPP7Q62tP7URaUkLDHwm0dnr4z4f89fDrBtr3H6lI7xHspeI9gi34n1EgInVk9fZ9fLJkKy/OXF+h3755WiKPXNGHV7/dyGfLQj/Xtl2zFN69/YzD7zNSg/b/ZwXp8mnwI4Hy5sKEH9V1FPVGpF1DTb0OREQOq+7m7pbdhVz81BcUFFeszZ8Yb6QlxTPiv7MBuOWsrhw4WMKr326s0pXzuyE9K3x31OAe9f9hLpEK1feflA7dz4eVn0BaS9h9oPZjq4civSK4DPjMObc78D4DOMc597Z3oYnEpnA3dy/tl8X6nfu5bcKcKkkA/KWbnYPbB3VjX1EJo4f2xMzo36lFtV05DeJhLpEK1fd/cB9s/Bba9oHLn4MnetVuXPWURVI1wszmOef6Vlo21znXz6vAQsnOznY5OaGrFYo0dKc//FnQLpr05AR+dEoHXv3GP3t3T2FJ0O8bsPbh2q1nX+/c1yzMZ7sPvw75uMjWje6+gJnNds5lB/ss0pvFwcpVH+mNZhEJyMsv4MEPlnJiVjN+dEoH9hWWhBynv6+ohBdnrmNAlxY8euVJDB87M2jbBjmM82iUFEHRXlj/FexYUf1N4PIa2cH+SEV6MM8xs8eBp/HfNP4VMNuzqERiwMESH1eMmcH2vUV8sHAzj3y0LGz7dhkpzBh93qH3japPP1KhzuDLa96ldmJpRCJNBL8C/gi8Fng/BfiDJxGJNGLlbwK3aJLEzv0HGXd9fxLijQ07/Tcui0p9PPnxyioH+N8Nrnhzt1H16UcqXBL4yXvQfgAkJMP9GbUWUmMQ6aih/cBoj2MRadQmfL2eB95fcugm7879BwHYU1DMldkdKrRt0zQlogN8gx/GGU1dzjr8upFPAIu2SEcNfQxc5ZzLD7xvDrzqnBvsYWwijcaW3YXc89aioBO2nvhkZZVEEHMH+Ehu2h6swVBP9f3XSKRdQ5llSQDAOfedmSm1igSEGve/Kb+Ax6esYNvewpCzdsM9zrHBC3eAv2MebFsGbXqF7vLZvw3evBn2boZNczwNNZZFmgh8ZtbRObcBwMw6E6QaqUgsCjXu/0BxCX+bvIJdgS6g1MT4oOWbG/Uon3AH+L+287/OCjqi8bBVn0Bmd+gzHGb/K7rxCRB5IrgH+NLMpgXenwWM8CYkkYYlVFG3Rz9cTn5BMa/cPJCN3x2gsLiUhyYta/ijfKI19n7QPZDcFD6+N3y7kSsgPlDeetkH6vv3QKQ3iz8ys2z8B/95wDtAI76eFYlcqK6d/IJiOrdMY2DXFpxmLQE4JiWx4Y/yCXeWX7gHDuyELQtg8/zw6zk78LD33sPhsa6h28WXe8aB+v49EenN4p8DdwDt8SeCgcBMKj66Mtj3hgD/AOKB551zD4dodwowC/iRc+6NSIMXqWs+nyMpIa7Cw9nL+2HfLMzs0PtGfxP44Y7UuNe4SUtPQpHIRdo1dAdwCjDLOTfIzHoC94f7gpnF45+Adj6QC3xrZu8655YEafcIMLmmwYvUta9W76CoxEdivFUo6gbQq21Tbj6zAU1uqq7Lp3A3rJkafh0Dfwmtj4c2J0CzjvC3bpFtW8M961SkiaDQOVdoZphZsnNumZlV17E5AFjlnFsDYGavAsOAJZXa/Qp4E3+iEWlQ3pmXR9PkBP50SS+e/GTloZIPcQYTbh5I04b06MZwXT7frfOXbd4efvYzQx6q+D7SA7y6fOpUpIkgN1Bx9G3gYzP7juofVZkFbCy/DuDU8g3MLAv/Yy/PJUwiMLMRBG5Od+yo5+FI/VBYXMrkRVsYfOKxXJXdgauyO/Dhws388uU5nNuzDRlpSXUdol+4M/2RKyB/PSz/KPw6njsPfCVwxQvw5k2Rb1sH+AYh0pvFlwVe3mdmnwPNgGr+z8GCLKvcefgkcJdzrrR8P2qQ7Y8DxoG/+mgkMYt4bebqnewtKuHiPm0PLTu1a0taN03muoH16IQl3Jn+CxdA7jfVr6PpsXDVfyCzG3x0t7pxGpkaVxB1zk2rvhXgvwIoP12yPVWvIrKBVwNJIBO40MxK9JwDqY9KfY7FebvpndUMM2Paiu2kJMYxsOvhm50tmiTxzT0/qMMoa+jADrjgQTjuXBhzWuh2t0z3P7sXdJbfCHlZSvpboLuZdQE2AVcD15Zv4Jw7dCfNzP4NvK8kIPXNnsJi/vnZKnLW7WLOhnzuufB4bj6rK9NXbGdg15akJMbXTWChunxSMuCudZC/Aeb+N/w6fj03sm3F1dE+Sq3wLBE450rM7Hb8o4HigfHOucVm9ovA52O92rZINJSVjSi7AZyRmkif9s14bPJy9hWVsGbHfq4b2KnuAgzV5VOYD0/2hn1b/f36kdLInZjl6cNlnHOTgEmVlgVNAM65n3oZi0hNVC4bAVBU4mN4dgdez9nIPz5dSc9jm3ozJyDczd3/WwJxEfyzbX+K/5m8Z9wJT5wQ2XbV5ROz9JQxkSBClY0YM3U1n488hy9Xbef7x2V60y0U7ubug8dCy27Q5ezw67iqXE0enelLNZQIJOaEqhRaxudzIctG5OUXkJQQx7k929R8w6HO9OOT4ZInYcMsKNoTfh2n/gIWTYRvno18uzrTl2ooEdQT1R2cAGav38W4aWvIWf8du/YfJLNpMhf0as0Dw3oTFxd6+K34rdq2l+emr+Gd+XkUBh4Osym/gNETF/DV6h08MOxEUhLjeeXbDSGLJBxVpdBQZ/qlRfD2L/2v048Nv47BD8J59/qf0RuuPo9IDSgR1ANvzc7l928vPPTkqrIyxs45hvXNIi7OeGnWev7w9qIK39u+t4iXv95Izrrv+O9Np9L6mJS6CL/e+njJVsZOW43P+Q/rS/L2BK0JVFjs4385uSzYuJvHrurD2GmraZ+RypY9hZT4DqcETyuFtjkRjhsE5z9Q/WMWE5IgoaW6fCRqzLmGNT8rOzvb5eTk1HUYUbNo024uffqrCgecMmlJ8XRv05R///QUzvnbVAqKSzkYorhZp5ZpTB15DuEm5sWSDTsPMPQf02mZnkynlmkAtG2Wwus5uSG/k5mezI59RQA8e31/Cg6WRlYpNJKyzAd2waNh6g7dmw9lv919zUK3u2936M9EwjCz2c65oA9/0BVBHbvj1blBkwDAgYOlzN+Yz2XPfMXuguKgU7XLrN95gE+XbmPdzv3cdEaXiBNCJF1SXon2tsuvLzkxDudzvDJiIFnlunO+WrXz0HDQ8rIyUnjn9jP4YMFm0pLiOf/4NsTFWWTxhLu5u+QdmPk0bF8efh3lfy+d6UstUyKoQxt3HWD19v00S01gd0Hw8d4X9j6W2eu/47J+WXyzdlfQg1irpsls31vEyDfmk3+gmLSkBK4+pQMFxaU0ST78Ezvn2FNYQrNUfyG0YE/WuuvNBTz+8XJeu+U02jbz7slZwbY9euICgBong+JSH2/N2cSf3l10qO+/sNhHQpzx7dpdZJVb36jBPaoMC01JjGPU4J5kpifzk+93Pso9q+T1GyDze9DzIpj3cmTf0c1dqWVxdR1ALJu2YjsAtw7qRmqlYYgGtGiSyNPXnszXv/8BT/yoL6MG96jSLjUxnrsG9yAx3sg/UAzAfe8u5py/TeW0hz5lb2HxobYT52xiwIOfsHbHfiD4EMmiEh8bdhXw4cIt0d7dCoJtu7DYx6OTq1a3fOrTFZzy4Cd0Gf0Bpz/8GW/P3VTh89+9sYC73lxwKAmUKfE5Hptc8Uz80n5ZPHR5b7IyUjEgKyOVhy/v491VUPZNcOssuPSZ0Gf0OtOXOqYrglr0/Bdr6NSyCef3akNhcSnvzs+jffNURpzZlTZNUyp0k/z8zC6c27N1lYeaAEG7U/49cx2LNu3hvkt68eWqHXyy1N+10Pu+KbRqmkzvdsfg8B/ox01fw0OX9w770PRpK7bzszO8qaX/9Oergl7ZAOTlF/L4xyvo1CKNS05qxy9fms2nyw53k5TdSAf/f4+1O/bz9rxNNXowfNQeDuMrBRf8ns0hFz9++LXO9KWe0s3iWrJi614ueGI6yQlxPHKFf2TKsi17+ePFvbgpCgfcuycu5NVvN5Bzzw/4YuUORk+seoYMHHqASrtmKZT4HNv2FlVp0yQ5npJSx/ifnkJqUjx922cc1fDUjbsOkJwQx80v5tAlswlvz8vDCP8cq4Q4o1vrdJZt2Ru2jc85EuLjaJGWxJY9hVXaZGWk8tXosA/SCy7UDWCLg+8NhRMvh/d/U/24f93clXpCN4vrUPl6NYb/4HXna/PITE/iXzeewqAe0ekWuPWc4zireyYt05N5bPLyoEkA4L4fnsC2PUX8Z+Y6miYnVHmyVmpiPDcM7MyYaav58fNfA3Bm90z+ftVJzFi9s8Y3d5du3sOwf34FwMFSH/Nzd3NMSgJFxaUcLHUVkkFqYhzDsztw2nGZ3P/e4qAH9vJuOds/jr5P+wwKDpZW6fs/quGeoW4AOx+s+wKWfwDNO8Npt8PMfwZPCOrykQZCicBDlW+IOqDUOW4fdBw3n3kczdKi9/SqDi3S6NDCP0wyXJfPD09qR9OURHoc25RbX56D4b9KKCl1hw7uw/q2Y8iJx1JQXMrSzXt45KNlnPXoZxwsdZQNcKrcRVN5v8sSRnyckZIYR9tmqVzYuy0zVu/gkpPaMfiEY5m6bBtPfbYqaGI5pXNz4uOMi576MsQon1RGDe5ZZXmtjIC6biJMuQeGPgLt+sE5d0V/GyK1SF1DHjr94c9CHsSOqLsiytv9v9fnMXHOJp69vj+DTwg9o3XVtn1c8v++rHJzt/I6nXM8+tFynv9yTYWrjMR447ErTzqig3Kw4m+pifE8dHnvIzvIRzLmHzSWXxqdcF1DGjXkoXD1arwUanRR5W6Sv17Wm5duOpULeoWvm9OtdTqFQZIAHN6X4lIft0+Yy5hpq6s8xL24tOronUgFG+VzRElg4Rvw8vDwY/4Bti6BcYOOKFaRhkpdQx5ql5Ea9Mz8qOrVRCDc6KLyUhLjOaN7ZkTrDLUvCfHG4rzdvDMvjw8Wbg75/aNJfkc9ymflx/5aPqUHw7ebfA+smQZ7Q++HSGOkROChUYN78Nv/zae0turVlBO1IZIBwSZiJcUbB0sdFz31JQCX98vi6xCT3rxOfiG7fMq07Oav3DlpZOg2M//p//vqCfDenZrdKzFDiSDKSn2ORz5axvePa8nSLXtIijd8ccbBEl+tl3CIplBXGZ0zm7BldyHJCXGc3i2TSQs3R3f0TqTCJYFL/gF9robElPCJ4PbZULgb2vf3zwQWiRFKBFH23BdrGDd9DS98uZZSn6N3VjNuP7db2JuxDUXIq4wOFdtALY3eiVT/n0bWLrObp2GI1FdKBFG0p7CYJz5ewWldW7Jw024Gdm3BczdkR68i6GPdYP/2qssrj3ipY9HulqpWcfj5BhWooJtIFUoEUfTRoi0Ulfj43ZAedGrZhGNSEiJLApEMaVz6fvAkAOG7RRo7Xyn876eRt69HCVOkvlAiiKL35ufRsUUafTtkRH4V4Fz4IY3PngU+H2xdGH49Y06HEdMgvgY/aaRj6r0Q6bZDtUtr6X+Iy9L3YMWH3sUpEgM0jyBK9hYW89WqHVzUp21kSaC4EJa8Cy9dEb5dk9ZwTFt/KYNwti6C1Z/BwQP+s+RIhEtABw9Eto4jFW7bfz8eJt4Cn/45dLsDO+GdW2H9DDj3D6rsKXIUdEUQJXM35ONzcFrXltU3LjkIL5wPWxZUf6C67o3Dr8uGNwaT1hLe/DkUhZn1WpMz/al/hQv+Elnb8kKdwSemwWVjIS0Tvn0u/Dra9YUFr1a/rdtzIKMjJCTDWaNqHquIAEoEUZOz/jviDPp1zKi+8Yx/+JPAsKf9wxofiCB5VKff9TDrGfj+r2HGU8Hb7N8Gm+f7x9TvWht+fTOfgeRmcPodsH0pvHRlZF05oc7giw/4H9ICkFDNs5WveQXmvgTxyTDx56HbZXYPvx4RiYgSQZTkrNvF8W2PoWlKNYXkNs2BqY/ACZdBv+tqtpFwI17O/aP/oJ3WInQiAP89B4sHV0330feGwOd/gfkTYNea0O1qcqP6po9h5ypo2xfGnBa+bdl/m3CJQESiQokgCrbtKWTuhnyGZ7cP37BwD7xxI6S3gYvKPbAk0iGN1XXrpLWoPtgzfwsH90OLrvDh70K3u2YCfPF3fz9910Gw5vPw6y34DqY9Gr5NhwH+PyJSrygRHKHypZaTEuIoKfVx/WmdQ3/BOXj/TsjfCDdOqnjQrs0hjef96fDr6Y+FnpcA/qRx4pX+fvj7M0Kvc/I9/qJuB3ZEHkekyU/j/kU8p0RwBCqXRi4q8ZEYbyzatJturdODf2nOi7DoTX8XTseBtRhtGKNWVd+meafq23wzDrqcDefcDc9HWF470uSncf8inlMiOALBHrxeVmq5wozaYCNoPnsAvn7W2wNcbZ9Fj1oNKcfUzbZF5KgpERyBiJ8zUF3te694kWTCHeDLkoBX2xYRTykRHIEWTZLYub9qbXvPSy3XJR3gRRotzSyuoa17Ctm1/yCV5w7X1nMGRESiTYmghqat2I4DRg7ucfSPTxQRqQfUNVRD01dsp3XTZG495zhuGxSmfn3u7NoLSkTkKHh6RWBmQ8xsuZmtMrPRQT7/sZktCPyZYWYneRnP0Sgp9fHIR8uYunw7Z32vVfjCcgX58NYIsBD/eTWCRkTqEc+uCMwsHngaOB/IBb41s3edc0vKNVsLnO2c+87MhgLjgFO9iulofLBwM2OmrqZdsxSuODnMDOKSg/DadfDdevjJ+9D59NoLUkTkCHjZNTQAWOWcWwNgZq8Cw4BDicA5N6Nc+1lANTUa6kZxqY8xU1fTrXU6U+48i7i4MFcDH/wG1n0Bl45VEhCRBsHLrqEsYGO597mBZaHcBAR9woiZjTCzHDPL2b49xFO6PLK3sJjhz85k2Za9/OrcbuGTwIav/VUzz/gN9L2m9oIUETkKXiaCYEdMF7Sh2SD8ieCuYJ8758Y557Kdc9mtWrWKYojhOee4/70lzN+Yz1PX9GNY3zB5zOfzzxpu0kq18UWkQfGyaygX6FDufXsgr3IjM+sDPA8Mdc7t9DCeiJQvJpeSGE9BcSm3D+rGD09qd7hRqIevgL+qaFKT2glWRCQKvEwE3wLdzawLsAm4Gri2fAMz6whMBK53zq3wMJaIVC4mV1BcSkKc0TWz0oE9XImI7J95GKGISPR5lgiccyVmdjswGYgHxjvnFpvZLwKfjwX+BLQEngkMxyxxzmVHO5byZ/ntMlIZNbhH0Mlfj05eVqWYXInP8fePV3B5/wjvY0f60HoRkXrC0wllzrlJwKRKy8aWe/1zwNNHUFU+y9+UX8BvX5/Pf2euo3f7DO78QXcy0pIAyMsvDLqOUEXmREQag0Y/szhYyehS55iXu9v/Z2M+hcWl/OyMLiHX0aiLyYnEiOLiYnJzcyksDH7C11ikpKTQvn17EhOreWxuOY0+EYQ6m/f5HKOG9ODRj5YTZ/C7NxaQnhxPic9RWOw71E7F5EQah9zcXJo2bUrnzp3DVwZowJxz7Ny5k9zcXLp0CX1yW1mjTwTtMlLZFCQZtMtI5ZdnH8f3j8tkf1EJd742j8eu7EP+geLQ9xOcg6/HVlnXISodIVJvFRYWNuokAGBmtGzZkprOt2r0iWDU4B4V7hHA4bN8M6NvhwwAvvn9eYf+BwlaRbTgO3jlGtgwE3pcBFc8p2GiIg1MY04CZY5kHxt9Iig7qFc3asjMQs8PaNIKWvWE3By45B/Q7waIUwVvEWkcGn0iAH8yiOhZASEfLbkd9u+Ay8dBn+HRDU5E6qVIh51HKj8/nwkTJnDrrbfW6HsXXnghEyZMICMj44i3XR2d1kbqqn8rCYjEiLJh55vyC3D4h53fPXEhb8/ddMTrzM/P55lnnqmyvLS0NEjrwyZNmuRpEoAYuSIIqaQINs2B9V/Byinh255waa2EJCLeu/+9xSzJ2xPy87kb8jlY6quwrKC4lN+9sYBXvtkQ9Du92h3DvZecEHKdo0ePZvXq1fTt25fExETS09Np27Yt8+bNY8mSJVx66aVs3LiRwsJC7rjjDkaMGAFA586dycnJYd++fQwdOpQzzjiDGTNmkJWVxTvvvENq6tEPb2/8iSBUv39cov/BMaVF/vetjq/duESk3qqcBKpbHomHH36YRYsWMW/ePKZOncpFF13EokWLDg3zHD9+PC1atKCgoIBTTjmFK664gpYtW1ZYx8qVK3nllVd47rnnGD58OG+++SbXXXfdEcdUpvEnglD9/r5iGHgbdPo+dDwNmrSE+5rVbmwiUifCnbkDnP7wZ0GHnWdlpPLaLadFJYYBAwZUGOv/1FNP8dZbbwGwceNGVq5cWSURdOnShb59+wLQv39/1q1bF5VYYvsewZC/wvEX+5MAhJ4HoPkBIjFl1OAepCbGV1gW7cmlTZocHn4+depUPvnkE2bOnMn8+fPp169f0BnQycnJh17Hx8dTUlISlVga/xVBTYxaWdcRiEg9EOmw85po2rQpe/fuDfrZ7t27ad68OWlpaSxbtoxZs2Yd8XaOhBKBiEgQEQ87j1DLli05/fTTOfHEE0lNTaVNmzaHPhsyZAhjx46lT58+9OjRg4EDB0Ztu5FQIhARqSUTJkwIujw5OZkPPwz6pN5D9wEyMzNZtGjRoeUjR46MWlyN/x6B+v1FRMJq/FcE6vcXEQmr8V8RiIhIWEoEIiIxTolARCTGKRGIiMS4xn+zWESkpkI+m6T1EQ9AOdIy1ABPPvkkI0aMIC0t7Yi2XR1dEYiIVBby2SQhlkcgVBnqSDz55JMcOHDgiLddHV0RiEjs+XA0bFl4ZN/910XBlx/bG4Y+HPJr5ctQn3/++bRu3ZrXX3+doqIiLrvsMu6//37279/P8OHDyc3NpbS0lD/+8Y9s3bqVvLw8Bg0aRGZmJp9//vmRxR2GEoGISC0oX4Z6ypQpvPHGG3zzzTc45/jhD3/I9OnT2b59O+3ateODDz4A/DWImjVrxuOPP87nn39OZmamJ7EpEYhI7Alz5g6EL0l/4wdHvfkpU6YwZcoU+vXrB8C+fftYuXIlZ555JiNHjuSuu+7i4osv5swzzzzqbUVCiUBEpJY557j77ru55ZZbqnw2e/ZsJk2axN13380FF1zAn/70J8/j0c1iEZHKPKhRVr4M9eDBgxk/fjz79u0DYNOmTWzbto28vDzS0tK47rrrGDlyJHPmzKnyXS/oikBEpDIPapSVL0M9dOhQrr32Wk47zf+0s/T0dF566SVWrVrFqFGjiIuLIzExkTFjxgAwYsQIhg4dStu2bT25WWzOuaiv1EvZ2dkuJyenrsMQkQZm6dKlHH98bDybPNi+mtls51x2sPbqGhIRiXFKBCIiMU6JQERiRkPrCj8SR7KPSgQiEhNSUlLYuXNno04Gzjl27txJSkpKjb6nUUMiEhPat29Pbm4u27dvr+tQPJWSkkL79u1r9B0lAhGJCYmJiXTp0qWuw6iXPO0aMrMhZrbczFaZ2eggn5uZPRX4fIGZnexlPCIiUpVnicDM4oGngaFAL+AaM+tVqdlQoHvgzwhgjFfxiIhIcF5eEQwAVjnn1jjnDgKvAsMqtRkGvOj8ZgEZZtbWw5hERKQSL+8RZAEby73PBU6NoE0WsLl8IzMbgf+KAWCfmS0/wpgygR1H+N36RvtSPzWWfWks+wHalzKdQn3gZSKwIMsqj9uKpA3OuXHAuKMOyCwn1BTrhkb7Uj81ln1pLPsB2pdIeNk1lAt0KPe+PZB3BG1ERMRDXiaCb4HuZtbFzJKAq4F3K7V5F7ghMHpoILDbObe58opERMQ7nnUNOedKzOx2YDIQD4x3zi02s18EPh8LTAIuBFYBB4AbvYon4Ki7l+oR7Uv91Fj2pbHsB2hfqtXgylCLiEh0qdaQiEiMUyIQEYlxMZMIqit3Ud+Z2TozW2hm88wsJ7CshZl9bGYrA383r+s4KzOz8Wa2zcwWlVsWMm4zuzvwGy03s8F1E3VwIfblPjPbFPhd5pnZheU+q8/70sHMPjezpWa22MzuCCxvUL9NmP1ocL+LmaWY2TdmNj+wL/cHlnv/mzjnGv0f/DerVwNdgSRgPtCrruOq4T6sAzIrLXsUGB14PRp4pK7jDBL3WcDJwKLq4sZfimQ+kAx0Cfxm8XW9D9Xsy33AyCBt6/u+tAVODrxuCqwIxNygfpsw+9Hgfhf886rSA68Tga+BgbXxm8TKFUEk5S4aomHAfwKv/wNcWnehBOecmw7sqrQ4VNzDgFedc0XOubX4R5MNqI04IxFiX0Kp7/uy2Tk3J/B6L7AU/6z+BvXbhNmPUOrlfgA4v32Bt4mBP45a+E1iJRGEKmXRkDhgipnNDpTcAGjjAvMuAn+3rrPoaiZU3A31d7o9UD13fLnL9gazL2bWGeiH/wy0wf42lfYDGuDvYmbxZjYP2AZ87Jyrld8kVhJBRKUs6rnTnXMn46/YepuZnVXXAXmgIf5OY4DjgL74a2T9PbC8QeyLmaUDbwJ3Ouf2hGsaZFm92Z8g+9EgfxfnXKlzri/+KgsDzOzEMM2jti+xkggafCkL51xe4O9twFv4LwG3llVrDfy9re4irJFQcTe438k5tzXwj9cHPMfhS/N6vy9mloj/4Pmyc25iYHGD+22C7UdD/l0AnHP5wFRgCLXwm8RKIoik3EW9ZWZNzKxp2WvgAmAR/n34SaDZT4B36ibCGgsV97vA1WaWbGZd8D+n4ps6iC9iVrFs+mX4fxeo5/tiZga8ACx1zj1e7qMG9duE2o+G+LuYWSszywi8TgV+ACyjNn6Tur5TXot35C/EP6JgNXBPXcdTw9i74h8dMB9YXBY/0BL4FFgZ+LtFXccaJPZX8F+aF+M/g7kpXNzAPYHfaDkwtK7jj2Bf/gssBBYE/mG2bSD7cgb+boQFwLzAnwsb2m8TZj8a3O8C9AHmBmJeBPwpsNzz30QlJkREYlysdA2JiEgISgQiIjFOiUBEJMYpEYiIxDglAhGRGKdEIOIxMzvHzN6v6zhEQlEiEBGJcUoEIgFmdl2gHvw8M3s2UABsn5n93czmmNmnZtYq0Lavmc0KFDV7q6yomZl1M7NPAjXl55jZcYHVp5vZG2a2zMxeDsyIxcweNrMlgfX8rY52XWKcEoEIYGbHAz/CX9yvL1AK/BhoAsxx/oJ/04B7A195EbjLOdcH/wzWsuUvA087504Cvo9/JjL4q2Leib+GfFfgdDNrgb/8wQmB9fzFy30UCUWJQMTvPKA/8G2gDPB5+A/YPuC1QJuXgDPMrBmQ4ZybFlj+H+CsQD2oLOfcWwDOuULn3IFAm2+cc7nOXwRtHtAZ2AMUAs+b2eVAWVuRWqVEIOJnwH+cc30Df3o45+4L0i5cTZZgZYHLFJV7XQokOOdK8FfFfBP/w0Y+qlnIItGhRCDi9ylwpZm1hkPPie2E/9/IlYE21wJfOud2A9+Z2ZmB5dcD05y/Dn6umV0aWEeymaWF2mCghn4z59wk/N1GfaO+VyIRSKjrAETqA+fcEjP7A/6nwMXhrzB6G7AfOMHMZgO78d9HAH854LGBA/0a4MbA8uuBZ83sz4F1XBVms02Bd8wsBf/VxG+ivFsiEVH1UZEwzGyfcy69ruMQ8ZK6hkREYpyuCEREYpyuCEREYpwSgYhIjFMiEBGJcUoEIiIxTolARCTG/X9xNKhhk3+H8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybRshQ63HAEG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}